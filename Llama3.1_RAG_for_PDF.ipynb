{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -qq install langchain\n",
        "!pip -qq install langchain-core\n",
        "!pip -qq install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ollama langchain beautifulsoup4 chromadb gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The question of the meaning of life has puzzled philosophers, theologians, scientists, and thinkers for centuries. While there's no one definitive answer, here are some perspectives on this profound topic:\\n\\n**Philosophical perspectives:**\\n\\n1. **Hedonism**: Life's purpose is to seek pleasure and avoid pain.\\n2. **Utilitarianism**: The meaning of life is to maximize happiness and minimize suffering for the greatest number of people.\\n3. **Existentialism**: Life has no inherent meaning; individuals must create their own purpose and values.\\n\\n**Spiritual and religious perspectives:**\\n\\n1. **Christianity**: The purpose of life is to love God and follow His commandments, with eternal salvation being the ultimate goal.\\n2. **Buddhism**: The meaning of life is to attain enlightenment (Nirvana) through meditation, self-awareness, and letting go of attachment to worldly desires.\\n3. **Islam**: Life's purpose is to worship Allah and live according to His will, as outlined in the Quran.\\n\\n**Scientific perspectives:**\\n\\n1. **Biological perspective**: The meaning of life is to survive, reproduce, and perpetuate one's genetic lineage.\\n2. **Evolutionary perspective**: Life's purpose is to adapt, evolve, and improve one's position within the natural environment.\\n\\n**Personal growth and self-actualization:**\\n\\n1. **Self-discovery**: Life's purpose is to understand oneself, find inner peace, and cultivate a sense of fulfillment.\\n2. **Personal growth**: The meaning of life is to learn, grow, and become the best version of oneself.\\n\\n**Other perspectives:**\\n\\n1. **Nihilism**: Life has no inherent meaning; everything is ultimately meaningless.\\n2. **Humanism**: The purpose of life is to live a good, moral, and fulfilling life, with an emphasis on human values and experiences.\\n3. **New Age spirituality**: Life's purpose is to awaken to one's true nature as a spiritual being, and to co-create a harmonious universe.\\n\\nAs you can see, the meaning of life is a highly subjective and context-dependent question, with various perspectives and interpretations across cultures, philosophies, and personal experiences. Ultimately, each individual must find their own answer to this profound query.\""
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model = \"llama3.1\")\n",
        "llm.invoke(\"what is the Meaning of life\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLNJWzd0Ul9V",
        "outputId": "2dae7d24-e1f3-401e-a4b8-a2db3bf9a6a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'success'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ollama\n",
        "ollama.pull(model=\"nomic-embed-text\")\n",
        "ollama.pull(model=\"llama3.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "import PyPDF2\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "# Load the data from the PDF\n",
        "pdf_path = 'https://arxiv.org/pdf/2404.11597'\n",
        "\n",
        "def load_pdf(pdf_path):\n",
        "    response = requests.get(pdf_path)\n",
        "    with open(\"document.pdf\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "        \n",
        "    text = \"\"\n",
        "    with open(\"document.pdf\", \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_text = load_pdf(pdf_path)\n",
        "docs = [Document(page_content=pdf_text, metadata={})]\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Create Ollama embeddings and vector store, ensuring CUDA usage\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "\n",
        "# Ensure that the embeddings use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. Answer the question based on the given context. \"\n",
        "        \"If the answer is not in the provided text, just respond with 'I do not know'.\"\n",
        "    )\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_message},\n",
        "        {'role': 'user', 'content': formatted_prompt}\n",
        "    ]\n",
        "    response = ollama.chat(model='llama3.1', messages=messages)\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "def rag_chain(question):\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    answer = ollama_llm(question, formatted_context)\n",
        "    return f\"Answer: {answer}\\n\\nUsed Text for Inference:\\n{formatted_context}\"\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(question):\n",
        "    return rag_chain(question)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "  outputs=\"text\",\n",
        "  title=\"RAG with Llama3.1\",\n",
        "  description=\"Ask questions about the provided context\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
