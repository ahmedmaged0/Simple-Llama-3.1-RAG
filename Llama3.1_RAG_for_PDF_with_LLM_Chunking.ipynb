{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -qq install langchain\n",
        "!pip -qq install langchain-core\n",
        "!pip -qq install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ollama langchain beautifulsoup4 chromadb gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"One of the most profound and intriguing questions humanity has ever pondered. The meaning of life is a complex and multifaceted concept that has been debated by philosophers, theologians, scientists, and scholars across various disciplines for centuries.\\n\\nThere isn't a single, universally accepted answer to this question. However, here are some perspectives on the meaning of life:\\n\\n**Philosophical Perspectives:**\\n\\n1. **Existentialism:** Life has no inherent meaning; it is up to each individual to create their own purpose.\\n2. **Hedonism:** The ultimate goal of life is to maximize happiness and pleasure.\\n3. **Stoicism:** The meaning of life lies in living a virtuous, self-disciplined life that aligns with reason and virtue.\\n\\n**Spiritual Perspectives:**\\n\\n1. **Theism:** The purpose of life is to serve God or achieve spiritual enlightenment.\\n2. **Buddhism:** The goal of life is to attain Nirvana (liberation from suffering) through the Four Noble Truths.\\n3. **Judaism:** Life has a purpose ascribed by God, which includes following His commandments and treating others with kindness.\\n\\n**Scientific Perspectives:**\\n\\n1. **Evolutionary Biology:** Life exists to perpetuate genetic material and pass on traits to future generations.\\n2. **Social Sciences:** The meaning of life is found in social relationships, cooperation, and contributing to the greater good.\\n\\n**Personal Perspectives:**\\n\\nUltimately, the meaning of life is subjective and unique to each individual. It may involve:\\n\\n1. Pursuing one's passions and interests.\\n2. Building meaningful relationships and connections with others.\\n3. Developing personal growth and self-awareness.\\n4. Creating something that leaves a lasting impact (e.g., art, music, literature).\\n5. Exploring the mysteries of existence and understanding the universe.\\n\\n**The Answer Might Be...**\\n\\nThere is no definitive answer to the meaning of life. It may be that the search for meaning itself is the purpose of life â€“ a journey of discovery, exploration, and growth.\\n\\nWhat's your take on the meaning of life?\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model = \"llama3.1\")\n",
        "llm.invoke(\"what is the Meaning of life\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLNJWzd0Ul9V",
        "outputId": "2dae7d24-e1f3-401e-a4b8-a2db3bf9a6a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'success'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ollama\n",
        "ollama.pull(model=\"nomic-embed-text\")\n",
        "ollama.pull(model=\"llama3.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Running on local URL:  http://127.0.0.1:7870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET http://127.0.0.1:7870/startup-events \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7870/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "import PyPDF2\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "# Load the data from the PDF\n",
        "pdf_path = 'https://arxiv.org/pdf/2404.11597'\n",
        "\n",
        "def load_pdf(pdf_path):\n",
        "    response = requests.get(pdf_path)\n",
        "    with open(\"document.pdf\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "        \n",
        "    text = \"\"\n",
        "    with open(\"document.pdf\", \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "pdf_text = load_pdf(pdf_path)\n",
        "docs = [Document(page_content=pdf_text, metadata={})]\n",
        "\n",
        "# Function to use LLM for chunking\n",
        "def llm_chunking(text, chunk_size):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. Split the given text into chunks. \"\n",
        "        \"Each chunk should be around the specified character size.\"\n",
        "    )\n",
        "    prompt = f\"Text: {text}\\n\\nChunk size: {chunk_size}\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_message},\n",
        "        {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "    response = ollama.chat(model='llama3.1', messages=messages)\n",
        "    # Process the response to extract the chunks\n",
        "    # Assuming the response is structured as a JSON object with 'chunks' as a list of text chunks\n",
        "    chunks = response['message']['content'].split(\"\\n\\n\")  # Split response by double newline\n",
        "    return chunks\n",
        "\n",
        "# Split the loaded documents into chunks using LLM\n",
        "chunk_size = 4000  # Define your chunk size\n",
        "chunks = llm_chunking(pdf_text, chunk_size)\n",
        "\n",
        "# Convert chunks to Document objects\n",
        "splits = [Document(page_content=chunk, metadata={}) for chunk in chunks]\n",
        "\n",
        "# Create Ollama embeddings and vector store, ensuring CUDA usage\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "\n",
        "# Ensure that the embeddings use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. Answer the question based on the given context. \"\n",
        "        \"If the answer is not in the provided text, just respond with 'I do not know'.\"\n",
        "    )\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system_message},\n",
        "        {'role': 'user', 'content': formatted_prompt}\n",
        "    ]\n",
        "    response = ollama.chat(model='llama3.1', messages=messages)\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "def rag_chain(question):\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    answer = ollama_llm(question, formatted_context)\n",
        "    return f\"Answer: {answer}\\n\\nUsed Text for Inference:\\n{formatted_context}\"\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(question):\n",
        "    return rag_chain(question)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "  outputs=\"text\",\n",
        "  title=\"RAG with Llama3.1\",\n",
        "  description=\"Ask questions about the provided context\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
